{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from num2words import num2words\n",
    "from utils import create_client_engine\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[11/23/21 00:44:08] </span><span style=\"color: #000080\">INFO    </span> Using PostgreSQL                                     <a href=\"file:///Users/mobasshirbhuia/Desktop/take_home_project/Clickstream-Datawarehousing/utils.py\"><span style=\"color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f\">:12</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fa0612529d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client, client_engine = create_client_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datacatalog for Datawarehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2021-01', 'clickstream-enwiki-2021-01.tsv.gz', 609189888, 31983002),\n",
       " ('2017-11', 'clickstream-enwiki-2017-11.tsv.gz', 1207233629, 25982519),\n",
       " ('2021-01', 'clickstream-jawiki-2021-01.tsv.gz', 325695363, 6609174),\n",
       " ('2021-01', 'clickstream-dewiki-2021-01.tsv.gz', 283118417, 5838401),\n",
       " ('2021-01', 'clickstream-ruwiki-2021-01.tsv.gz', 364606675, 4956355),\n",
       " ('2017-11', 'clickstream-dewiki-2017-11.tsv.gz', 7634944, 4888888),\n",
       " ('2017-12', 'clickstream-dewiki-2017-12.tsv.gz', 209305165, 4842717),\n",
       " ('2021-01', 'clickstream-frwiki-2021-01.tsv.gz', 216488681, 4639229),\n",
       " ('2021-01', 'clickstream-eswiki-2021-01.tsv.gz', 192769729, 4203548),\n",
       " ('2021-01', 'clickstream-itwiki-2021-01.tsv.gz', 91308032, 3816020),\n",
       " ('2017-11', 'clickstream-eswiki-2017-11.tsv.gz', 136339852, 3127870),\n",
       " ('2017-11', 'clickstream-ruwiki-2017-11.tsv.gz', 154513011, 2507978),\n",
       " ('2021-01', 'clickstream-zhwiki-2021-01.tsv.gz', 91926642, 2229375),\n",
       " ('2021-01', 'clickstream-plwiki-2021-01.tsv.gz', 98246153, 2152793),\n",
       " ('2017-11', 'clickstream-jawiki-2017-11.tsv.gz', 96536358, 2066174),\n",
       " ('2021-01', 'clickstream-ptwiki-2021-01.tsv.gz', 82527695, 1768034),\n",
       " ('2021-01', 'clickstream-fawiki-2021-01.tsv.gz', 70403252, 1192129)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute(\n",
    "    \"\"\"\n",
    "    SELECT folder_name, file_name, size_in_bytes, number_of_rows FROM \"datacatalog\"\n",
    "    WHERE number_of_rows > 0 AND size_in_bytes > 0 AND data_inserted = True\n",
    "    ORDER BY number_of_rows DESC\n",
    "    \"\"\"\n",
    ")\n",
    "client.fetch_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total rows inserted into the datawarehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one hundred and twelve million, eight hundred and four thousand, two hundred and sixth'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute(\n",
    "    \"\"\"\n",
    "    SELECT SUM(number_of_rows) FROM \"datacatalog\"\n",
    "    WHERE number_of_rows > 0 AND size_in_bytes > 0 AND data_inserted = True\n",
    "    \"\"\"\n",
    ")\n",
    "num2words(int(client.fetch_all()[0][0]), to='ordinal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A\n",
    "In this take home project we're going to be analyzing wikipedia clickstream data found here: [link](https://dumps.wikimedia.org/other/clickstream/readme.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the dates of 2021 January, determine the top 5 \"current site\" requested\n",
    "and print out their counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Main_Page', Decimal('191524141')),\n",
       " ('メインページ', Decimal('20725997')),\n",
       " ('Заглавная_страница', Decimal('16634310')),\n",
       " ('Pagina_principale', Decimal('15842122')),\n",
       " ('Hyphen-minus', Decimal('15164165'))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute(\n",
    "    \"\"\"\n",
    "    SELECT resource, SUM(number_of_occurrences) as total_number_of_occurrences\n",
    "    FROM \"clickstream\"\n",
    "    WHERE date = '2021-01-01'\n",
    "    GROUP BY resource\n",
    "    ORDER BY total_number_of_occurrences DESC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    ")\n",
    "client.fetch_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In 2017 November, how many times was \"Ferrimagnetismus\" requested?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows it has as a resource\n",
    "# client.execute(\n",
    "#     \"\"\"\n",
    "#     SELECT COUNT(*) FROM \"clickstream\"\n",
    "#     WHERE date = '2017-11-01' AND resource = 'Ferrimagnetismus'\n",
    "#     \"\"\"\n",
    "# )\n",
    "# client.fetch_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ferrimagnetismus', Decimal('1237'))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute(\n",
    "    \"\"\"\n",
    "    SELECT resource, SUM(number_of_occurrences) as total_number_of_occurrences \n",
    "    FROM \"clickstream\"\n",
    "    WHERE date = '2017-11-01' AND resource = 'Ferrimagnetismus'\n",
    "    GROUP BY resource\n",
    "    \"\"\"\n",
    ")\n",
    "client.fetch_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In 2017 November, how many events had \"other-search\" as their source?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6493854,)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) FROM \"clickstream\"\n",
    "    WHERE date = '2017-11-01' AND referrer = 'other-search'\n",
    "    \"\"\"\n",
    ")\n",
    "client.fetch_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues faced\n",
    "1. Had to wrangle millions of rows with limited resources both interms of time and memory.\n",
    "2. Among all one file has two rows with more than 500 varying characters (exact max value15912897) which I have skipped and but logged and save to a file for further analysis.\n",
    "3. Used pandas for data processing and analysis which could have been better with `Pyspark` which is a more efficient data processing and analysis ecosystem.\n",
    "4. For this dataset of almost 108M rows, pandas data wrangling and dump to Postgres was good enough but more than that is beyond the scope of this take home project architecture.\n",
    "\n",
    "## Fun facts\n",
    "1. Learned and explored pandas and postgres database's capabilities to work with large datasets.\n",
    "2. Learned and explored Spark's capabilities to work with large datasets.\n",
    "3. Implemented a robust data pipeline with logging and error handling managed by apache airflow.\n",
    "4. Implemented wrapper classes and functions to make the code more readable and maintainable.\n",
    "5. Maintaining data catalog for data lake creation, basic data cleaning and populating data warehouse was a fun.    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af0cda37f94d2db2110c46ee8ee12bf72605dab3961997a6203a365f0dc81ab3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cdw': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
